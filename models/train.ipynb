{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def load_data(database_filepath):\n",
    "    pass\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    pass\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    pass\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, Y_test, category_names):\n",
    "    pass\n",
    "\n",
    "\n",
    "def save_model(model, model_filepath):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "#from sklearn.multioutput import MultiOutputClassifier\n",
    "#from sklearn.metrics import classification_report\n",
    "#from sklearn.ensemble import AdaBoostClassifier\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/bambar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(database_filepath):\n",
    "    \"\"\"\n",
    "    Import dataseet from SQLite database.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        database_filepath: str\n",
    "\n",
    "    Returns:\n",
    "        pandas.Series, list : X, Y, category_names.\n",
    "    \"\"\"\n",
    "\n",
    "    # load data from sqlite Database\n",
    "    engine = create_engine(f'sqlite:///{database_filepath}.db')\n",
    "\n",
    "    sql_script = \"SELECT * FROM categories;\"\n",
    "\n",
    "    df = pd.read_sql(sql_script, engine)\n",
    "\n",
    "    print(df.head())\n",
    "    \n",
    "    # features and target\n",
    "    X = df.message.values\n",
    "    Y = df.genre.values    \n",
    "\n",
    "    category_names = list(df.columns)[3:]\n",
    "    \n",
    "    return X, Y, category_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize text (normalize, lemmatize, tokenize)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        text:str to be tokenized.\n",
    "\n",
    "    Return:\n",
    "        list:text tokens.     \n",
    "    \"\"\"\n",
    "    # Eliminate punctations\n",
    "    text =  ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    #tokenize text\n",
    "    tokens = word_tokenize(text,preserve_line=True)\n",
    "\n",
    "    # initiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for token_x in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(token_x).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= '''Build and return a machine learning model. Returns, Model, Machine learning model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['build',\n",
       " 'and',\n",
       " 'return',\n",
       " 'a',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'model',\n",
       " 'returns',\n",
       " 'model',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'model']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "    Build and return a machine learning model.\n",
    "     Input:\n",
    "       None\n",
    "    Return:\n",
    "        Model: Machine learning model.\n",
    "    \"\"\"\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ]))\n",
    "        ])),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "    # grid search parameters\n",
    "    parameters = {\n",
    "        'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "        'clf__n_estimators': [50, 100, 200],\n",
    "        'clf__min_samples_split': [2, 3, 4]\n",
    "    }\n",
    "    #create grid search object\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, Y_test, category_names):\n",
    "    \"\"\"\n",
    "    Evaluate ML Model performance on test data\n",
    "\n",
    "    Args:\n",
    "        model: Machine learning model.\n",
    "        X_test: Test set variables.\n",
    "        Y_test: Target Variable for test set.\n",
    "        category_names: List of categories.\n",
    "\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    Y_pred = model.predict(X_test)\n",
    "    confusion_mat = confusion_matrix(Y_test, Y_pred, labels=category_names)\n",
    "    accuracy = (Y_pred == Y_test).mean()\n",
    "    \n",
    "    print(\"Categories: \\n\", category_names)\n",
    "    print(\"Model Accuracy: \", accuracy)\n",
    "    print(\"Confusion Matrix: \\n\", confusion_mat)\n",
    "    print(\"\\n Top Important Features: \", model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_filepath):\n",
    "    \"\"\"\n",
    "    Save model to as pickle file.\n",
    "\n",
    "    Input:\n",
    "        model: Machine learning model.\n",
    "        model_filepath (str): Location/Filepath to save the model.\n",
    "\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(model_filepath, 'wb') as f:\n",
    "        pickle.dump(model, f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to train and evaluate a machine learning model.\n",
    "\n",
    "    Reads command line arguments, loads data, trains the model, evaluates it, and saves the       model.\n",
    "    \"\"\"\n",
    "    if len(sys.argv) == 3:\n",
    "        database_filepath, model_filepath = sys.argv[1:]\n",
    "        print('Loading data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "        X, Y, category_names = load_data(database_filepath)\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "        \n",
    "        print('Building model...')\n",
    "        model = build_model()\n",
    "        \n",
    "        print('Training model...')\n",
    "        model.fit(X_train, Y_train)\n",
    "        \n",
    "        print('Evaluating model...')\n",
    "        evaluate_model(model, X_test, Y_test, category_names)\n",
    "\n",
    "        print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n",
    "        save_model(model, model_filepath)\n",
    "\n",
    "        print('Trained model saved!')\n",
    "\n",
    "    else:\n",
    "        print('Please provide the filepath of the disaster messages database '\\\n",
    "              'as the first argument and the filepath of the pickle file to '\\\n",
    "              'save the model to as the second argument. \\n\\nExample: python '\\\n",
    "              'train_classifier.py ../data/DisasterResponse.db classifier.pkl')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
